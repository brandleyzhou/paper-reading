
## Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics

### CVPR 2018

---

Alex Kendall<sup>1</sup>, Yarin Gal<sup>2</sup>, Roberto Cipolla<sup>1</sup>

<sup>1</sup> University of Cambridge, <sup>2</sup> University of Oxford

---

## Resources

[Paper](https://arxiv.org/pdf/1705.07115.pdf)

[Code](https://github.com/yaringal/multi-task-learning-example)

[Alex Kendall's Homepage](https://alexgkendall.com/research/)


---

## Research Goal

Learn a 3D reconstruction of an object from single-view images without supervision

---

#### Prior Work  

- Structure from motion (SfM)
  - Traditionally requires multiple views of a rigid object
  - Non-rigid SfM requires 2D landmarks 
- Shape from shading/symmetry
- Category-specific reconstruction
  - Priors from data (eg. SMPL for human bodies)
  - Difficult to obtain for all categories (eg. cats)


---

## Key Idea

Assume that the object is symmetric.

Motivation: With a fully symmetric object, the image could be mirrored and stereo reconstruction techniques could be used.

---

#### But they are not symmetric

- Shape
  - Hair styles, expression
- Appearance
  - Albedo (eg. asymmetric texture of cat faces, freckles)
  - Illumination

---

#### This is addressed 

- Illumination is explicitly modelled
- The model predicts, along with other factors, the probability that each pixel has a symmetric counterpart in the image

---

## Method

---
Train a model $\Phi$ to decompose an input image I into depth, albedo, viewpoint and lighting, together with a pair of confidence maps.

---

![overview](assets/overview.png)


---

In more detail:

An RGB image **I** is roughly centered on the object of interest. 

The function, $\Phi$, maps **I** to four factors $(d,a,w,l)$:

- depth map, $d: \Omega \to \mathbb{R}_+$
- albedo image, $a: \Omega \to \mathbb{R}^3$
- global light direction, $l\in\mathbb{S}^2$
- viewpoint, $w\in\mathbb{R}^6$

---

An image can be reconstructed from these factors:

$$\hat{\mathbf{I}} = \Pi(\Lambda(a,d,l),d,w)$$

where $\Lambda$ computes lighting, and $\Pi$ computes reprojection.

---

## Probably Symmetric Objects

Assume that depth and albedo are symmetric about a fixed vertical plane.

Then, they require that:

$$d \approx \text{flip} (d')$$

and

$$a \approx \text{flip} (a')$$

---

In practice, these constraints are used in training by computing a second reconstructed image:

$$\hat{\mathbf{I}}' = \Pi(\Lambda(a',d',l),d',w)$$


---

The model is trained to encourage both 
$\mathbf{I} \approx \hat{\mathbf{I}}$ and 
$\mathbf{I} \approx \hat{\mathbf{I}}'$

---

## Loss Function

![overview](assets/loss.png)

The same loss is calculated for the symmetric reconstruction, $\mathcal{L}(\hat{\mathbf{I}}', \mathbf{I}, \sigma')$.

---

## Loss Function

![overview](assets/fullloss.png)


---

## Image Formation

Nothing unusual.

- Perspective camera with assumed narrow field of view
- Viewpoint represents a rotation and translation along $x,y,z$
- Normals are computed, multiplied by light direction and added to ambient illumination.
- Above is multiplied by albedo to get illuminated texture.
- Light is represented as a spherical sector


---

#### Implementation

- Depth and albedo are generated by (separate) encoder-decoder networks
  - encoder-decoders do not use skip connections because input and output images are not spatially aligned (since the output is in the canonical viewpoint)
- Viewpoint and lighting are regressed using simple encoder networks
- Trained using Adam, 50k iterations

---

![overview](assets/overview.png)

---

# Experiments

---

#### Data

- Human face datasets
  - CelebA, 3DFAW and BFM
- Cats
  - Combine two datasets, one with 10k examples and another with 1.2k
- Cars
  - Synthetic cars rendered from ShapeNet

---

#### Results

![results](assets/results.png)<!-- .element height="50%" width="50%" -->

SIDE: Scale-invariant depth error

MAD: Mean angle deviation (normals)

---

#### Qualitative Results

![results](assets/qualitative1.png)<!-- .element height="70%" width="70%" -->



---

#### Qualitative Results

![results](assets/qualitative2.png)<!-- .element height="70%" width="70%" -->

---

#### Limitations

![results](assets/limitations.png)<!-- .element height="70%" width="70%" -->

- Fails under extreme lighting
  - assumes Lambertian shading model (ignores shadow and specularity)
- Fails with dark, noisy textures
- Fails with extreme angles

---

# Questions

